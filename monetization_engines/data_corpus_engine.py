"""
æ‰§è¡Œ1ï¼šç»“æ„åŒ–è¡Œä¸šè¯­æ–™åº“ç”Ÿæˆå™¨ (Data as a Service)
ç›®æ ‡å®¢æˆ·ï¼šå¼€å‘å‚ç›´é¢†åŸŸRAGå’Œå¾®è°ƒæ¨¡å‹çš„AIå›¢é˜Ÿ
å®šä»·ï¼š$500-$2000/åº“
"""

import asyncio
import json
import re
from typing import List, Dict, Any
from datetime import datetime
from pathlib import Path
import hashlib

class DataCorpusEngine:
    """
    é«˜ä»·å€¼è¡Œä¸šæ•°æ®æŠ“å–ã€æ¸…æ´—ã€å‘é‡åŒ–é¢„å¤„ç†å¼•æ“
    """
    
    def __init__(self, output_dir: str = "./data_products"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # é«˜ä»·å€¼è¡Œä¸šç›®æ ‡
        self.target_industries = {
            "medical_compliance": {
                "ä»·å€¼": "$2000/åº“",
                "æ•°æ®æº": [
                    "FDAå…¬å‘Š",
                    "åŒ»ç–—å™¨æ¢°æ³¨å†Œæ–‡æ¡£",
                    "ä¸´åºŠè¯•éªŒæŠ¥å‘Š",
                    "è¯å“è¯´æ˜ä¹¦"
                ],
                "ç—›ç‚¹": "åŒ»ç–—AIéœ€è¦åˆè§„æ•°æ®è®­ç»ƒï¼Œä½†æ•°æ®åˆ†æ•£ä¸”æ ¼å¼æ··ä¹±"
            },
            "legal_cases": {
                "ä»·å€¼": "$1500/åº“",
                "æ•°æ®æº": [
                    "æ³•é™¢åˆ¤å†³ä¹¦",
                    "æ³•å¾‹æ³•è§„",
                    "å¾‹å¸ˆå‡½æ¨¡æ¿",
                    "åˆåŒèŒƒæœ¬"
                ],
                "ç—›ç‚¹": "æ³•å¾‹AIéœ€è¦æ¡ˆä¾‹æ•°æ®ï¼Œä½†çˆ¬å–å’Œç»“æ„åŒ–æå…¶å›°éš¾"
            },
            "financial_reports": {
                "ä»·å€¼": "$1000/åº“",
                "æ•°æ®æº": [
                    "ä¸Šå¸‚å…¬å¸è´¢æŠ¥",
                    "åˆ¸å•†ç ”æŠ¥",
                    "è¡Œä¸šåˆ†ææŠ¥å‘Š",
                    "å®è§‚ç»æµæ•°æ®"
                ],
                "ç—›ç‚¹": "é‡‘èAIéœ€è¦ä¸“ä¸šæ•°æ®ï¼Œä½†Bloombergå¤ªè´µ"
            },
            "technical_docs": {
                "ä»·å€¼": "$800/åº“",
                "æ•°æ®æº": [
                    "APIæ–‡æ¡£",
                    "æŠ€æœ¯ç™½çš®ä¹¦",
                    "å¼€æºé¡¹ç›®æ–‡æ¡£",
                    "Stack Overflowç²¾é€‰"
                ],
                "ç—›ç‚¹": "ä»£ç åŠ©æ‰‹éœ€è¦é«˜è´¨é‡æŠ€æœ¯æ–‡æ¡£è®­ç»ƒ"
            }
        }
    
    async def crawl_industry_data(self, industry: str, max_docs: int = 10000) -> List[Dict]:
        """
        çˆ¬å–ç‰¹å®šè¡Œä¸šæ•°æ®
        """
        print(f"[æ•°æ®æŠ“å–] å¼€å§‹çˆ¬å– {industry} è¡Œä¸šæ•°æ®...")
        
        # æ¨¡æ‹Ÿçˆ¬å–é€»è¾‘ï¼ˆå®é™…éœ€è¦æ¥å…¥ä½ çš„OpenClawç³»ç»Ÿï¼‰
        raw_data = []
        
        if industry == "medical_compliance":
            raw_data = await self._crawl_fda_data(max_docs)
        elif industry == "legal_cases":
            raw_data = await self._crawl_legal_data(max_docs)
        elif industry == "financial_reports":
            raw_data = await self._crawl_financial_data(max_docs)
        elif industry == "technical_docs":
            raw_data = await self._crawl_technical_data(max_docs)
        
        print(f"[æ•°æ®æŠ“å–] å®Œæˆï¼Œå…±æŠ“å– {len(raw_data)} æ¡åŸå§‹æ•°æ®")
        return raw_data
    
    async def _crawl_fda_data(self, max_docs: int) -> List[Dict]:
        """çˆ¬å–FDAæ•°æ®ï¼ˆç¤ºä¾‹ï¼‰"""
        # å®é™…å®ç°ï¼šè°ƒç”¨OpenClawçˆ¬å–FDAç½‘ç«™
        return [
            {
                "url": f"https://www.fda.gov/doc_{i}",
                "title": f"FDA Compliance Document {i}",
                "content": f"Sample FDA compliance content {i}...",
                "date": "2024-01-01",
                "category": "medical_device"
            }
            for i in range(min(100, max_docs))  # ç¤ºä¾‹æ•°æ®
        ]
    
    async def _crawl_legal_data(self, max_docs: int) -> List[Dict]:
        """çˆ¬å–æ³•å¾‹æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰"""
        return [
            {
                "url": f"https://court.gov/case_{i}",
                "title": f"åˆ¤å†³ä¹¦ {i}",
                "content": f"æ¡ˆä»¶è¯¦æƒ… {i}...",
                "date": "2024-01-01",
                "category": "civil_case"
            }
            for i in range(min(100, max_docs))
        ]
    
    async def _crawl_financial_data(self, max_docs: int) -> List[Dict]:
        """çˆ¬å–é‡‘èæ•°æ®ï¼ˆç¤ºä¾‹ï¼‰"""
        return [
            {
                "url": f"https://finance.com/report_{i}",
                "title": f"ç ”æŠ¥ {i}",
                "content": f"åˆ†æå†…å®¹ {i}...",
                "date": "2024-01-01",
                "category": "research_report"
            }
            for i in range(min(100, max_docs))
        ]
    
    async def _crawl_technical_data(self, max_docs: int) -> List[Dict]:
        """çˆ¬å–æŠ€æœ¯æ–‡æ¡£ï¼ˆç¤ºä¾‹ï¼‰"""
        return [
            {
                "url": f"https://docs.example.com/api_{i}",
                "title": f"API Documentation {i}",
                "content": f"Technical content {i}...",
                "date": "2024-01-01",
                "category": "api_docs"
            }
            for i in range(min(100, max_docs))
        ]
    
    def clean_and_structure(self, raw_data: List[Dict]) -> List[Dict]:
        """
        æ•°æ®æ¸…æ´—å’Œç»“æ„åŒ–
        è¿™æ˜¯æ ¸å¿ƒä»·å€¼ï¼šAIå›¢é˜Ÿæœ€ç—›æ¨çš„è„æ´»
        """
        print(f"[æ•°æ®æ¸…æ´—] å¼€å§‹æ¸…æ´— {len(raw_data)} æ¡æ•°æ®...")
        
        cleaned_data = []
        for item in raw_data:
            cleaned = {
                "id": self._generate_id(item),
                "title": self._clean_text(item.get("title", "")),
                "content": self._clean_text(item.get("content", "")),
                "metadata": {
                    "source_url": item.get("url", ""),
                    "date": item.get("date", ""),
                    "category": item.get("category", ""),
                    "word_count": len(item.get("content", "").split()),
                    "quality_score": self._calculate_quality_score(item)
                },
                "processed_at": datetime.now().isoformat()
            }
            
            # åªä¿ç•™é«˜è´¨é‡æ•°æ®
            if cleaned["metadata"]["quality_score"] > 0.6:
                cleaned_data.append(cleaned)
        
        print(f"[æ•°æ®æ¸…æ´—] å®Œæˆï¼Œä¿ç•™ {len(cleaned_data)} æ¡é«˜è´¨é‡æ•°æ®")
        return cleaned_data
    
    def _generate_id(self, item: Dict) -> str:
        """ç”Ÿæˆå”¯ä¸€ID"""
        content = f"{item.get('url', '')}{item.get('title', '')}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _clean_text(self, text: str) -> str:
        """æ–‡æœ¬æ¸…æ´—"""
        # å»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        # å»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        # å»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()ï¼ˆï¼‰ã€‚ï¼Œï¼ï¼Ÿï¼›ï¼š]', '', text)
        return text.strip()
    
    def _calculate_quality_score(self, item: Dict) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°"""
        score = 0.0
        content = item.get("content", "")
        
        # é•¿åº¦æ£€æŸ¥
        if len(content) > 100:
            score += 0.3
        
        # ç»“æ„åŒ–ç¨‹åº¦
        if item.get("title") and item.get("date"):
            score += 0.2
        
        # å†…å®¹ä¸°å¯Œåº¦
        if len(content.split()) > 50:
            score += 0.3
        
        # æ¥æºå¯é æ€§
        if any(domain in item.get("url", "") for domain in ["gov", "edu", "org"]):
            score += 0.2
        
        return min(score, 1.0)
    
    def vectorize_for_rag(self, cleaned_data: List[Dict]) -> Dict[str, Any]:
        """
        å‘é‡åŒ–é¢„å¤„ç†ï¼ˆä¸ºRAGå‡†å¤‡ï¼‰
        è¿™æ˜¯ç¬¬äºŒå±‚ä»·å€¼ï¼šç›´æ¥å¯ç”¨äºå‘é‡æ•°æ®åº“
        """
        print(f"[å‘é‡åŒ–] å¼€å§‹å‘é‡åŒ– {len(cleaned_data)} æ¡æ•°æ®...")
        
        vectorized_corpus = {
            "documents": [],
            "metadata": {
                "total_docs": len(cleaned_data),
                "avg_length": sum(d["metadata"]["word_count"] for d in cleaned_data) / len(cleaned_data),
                "created_at": datetime.now().isoformat(),
                "format": "ready_for_embedding",
                "recommended_models": ["text-embedding-3-large", "bge-large-zh"]
            }
        }
        
        for doc in cleaned_data:
            # åˆ†å—å¤„ç†ï¼ˆChunkingï¼‰
            chunks = self._chunk_text(doc["content"], chunk_size=512, overlap=50)
            
            for i, chunk in enumerate(chunks):
                vectorized_corpus["documents"].append({
                    "doc_id": doc["id"],
                    "chunk_id": f"{doc['id']}_chunk_{i}",
                    "text": chunk,
                    "metadata": {
                        **doc["metadata"],
                        "chunk_index": i,
                        "total_chunks": len(chunks)
                    }
                })
        
        print(f"[å‘é‡åŒ–] å®Œæˆï¼Œç”Ÿæˆ {len(vectorized_corpus['documents'])} ä¸ªæ–‡æœ¬å—")
        return vectorized_corpus
    
    def _chunk_text(self, text: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:
        """æ–‡æœ¬åˆ†å—"""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            if chunk:
                chunks.append(chunk)
        
        return chunks
    
    def package_as_product(self, industry: str, vectorized_corpus: Dict) -> str:
        """
        æ‰“åŒ…ä¸ºå¯å”®å–çš„æ•°æ®äº§å“
        """
        product_name = f"{industry}_corpus_{datetime.now().strftime('%Y%m%d')}"
        product_dir = self.output_dir / product_name
        product_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜æ•°æ®
        data_file = product_dir / "corpus.json"
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(vectorized_corpus, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆREADME
        readme = self._generate_product_readme(industry, vectorized_corpus)
        readme_file = product_dir / "README.md"
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(readme)
        
        # ç”Ÿæˆä½¿ç”¨ç¤ºä¾‹
        example = self._generate_usage_example(industry)
        example_file = product_dir / "usage_example.py"
        with open(example_file, 'w', encoding='utf-8') as f:
            f.write(example)
        
        print(f"[æ‰“åŒ…å®Œæˆ] æ•°æ®äº§å“å·²ä¿å­˜è‡³: {product_dir}")
        return str(product_dir)
    
    def _generate_product_readme(self, industry: str, corpus: Dict) -> str:
        """ç”Ÿæˆäº§å“è¯´æ˜æ–‡æ¡£"""
        info = self.target_industries.get(industry, {})
        
        return f"""# {industry.upper()} è¡Œä¸šè¯­æ–™åº“

## äº§å“ä¿¡æ¯
- **è¡Œä¸š**: {industry}
- **æ–‡æ¡£æ•°é‡**: {corpus['metadata']['total_docs']}
- **æ–‡æœ¬å—æ•°é‡**: {len(corpus['documents'])}
- **å¹³å‡é•¿åº¦**: {corpus['metadata']['avg_length']:.0f} è¯
- **ç”Ÿæˆæ—¶é—´**: {corpus['metadata']['created_at']}
- **å®šä»·**: {info.get('ä»·å€¼', 'N/A')}

## æ•°æ®æ¥æº
{chr(10).join(f"- {source}" for source in info.get('æ•°æ®æº', []))}

## å®¢æˆ·ç—›ç‚¹
{info.get('ç—›ç‚¹', '')}

## æ•°æ®æ ¼å¼
```json
{{
  "doc_id": "å”¯ä¸€æ–‡æ¡£ID",
  "chunk_id": "æ–‡æœ¬å—ID",
  "text": "å·²æ¸…æ´—çš„æ–‡æœ¬å†…å®¹",
  "metadata": {{
    "source_url": "åŸå§‹URL",
    "date": "å‘å¸ƒæ—¥æœŸ",
    "category": "åˆ†ç±»",
    "quality_score": "è´¨é‡åˆ†æ•°"
  }}
}}
```

## æ¨èä½¿ç”¨åœºæ™¯
1. RAGç³»ç»Ÿè®­ç»ƒæ•°æ®
2. é¢†åŸŸæ¨¡å‹å¾®è°ƒ
3. çŸ¥è¯†å›¾è°±æ„å»º
4. è¯­ä¹‰æœç´¢å¼•æ“

## æ¨èå‘é‡æ¨¡å‹
- OpenAI: text-embedding-3-large
- å¼€æº: bge-large-zh / bge-large-en

## æŠ€æœ¯æ”¯æŒ
è´­ä¹°åæä¾›30å¤©æŠ€æœ¯æ”¯æŒï¼ŒåŒ…æ‹¬æ•°æ®æ›´æ–°å’Œå®šåˆ¶åŒ–æ¸…æ´—ã€‚
"""
    
    def _generate_usage_example(self, industry: str) -> str:
        """ç”Ÿæˆä½¿ç”¨ç¤ºä¾‹ä»£ç """
        return f'''"""
{industry} è¯­æ–™åº“ä½¿ç”¨ç¤ºä¾‹
"""

import json
from openai import OpenAI

# 1. åŠ è½½è¯­æ–™åº“
with open('corpus.json', 'r', encoding='utf-8') as f:
    corpus = json.load(f)

print(f"åŠ è½½äº† {{len(corpus['documents'])}} ä¸ªæ–‡æœ¬å—")

# 2. å‘é‡åŒ–ï¼ˆä½¿ç”¨OpenAI Embeddingï¼‰
client = OpenAI(api_key="your-api-key")

embeddings = []
for doc in corpus['documents'][:10]:  # ç¤ºä¾‹ï¼šåªå¤„ç†å‰10ä¸ª
    response = client.embeddings.create(
        model="text-embedding-3-large",
        input=doc['text']
    )
    embeddings.append({{
        'chunk_id': doc['chunk_id'],
        'embedding': response.data[0].embedding,
        'metadata': doc['metadata']
    }})

print(f"ç”Ÿæˆäº† {{len(embeddings)}} ä¸ªå‘é‡")

# 3. å­˜å…¥å‘é‡æ•°æ®åº“ï¼ˆä»¥Pineconeä¸ºä¾‹ï¼‰
import pinecone

pinecone.init(api_key="your-pinecone-key")
index = pinecone.Index("{industry}_index")

# æ‰¹é‡æ’å…¥
vectors = [
    (e['chunk_id'], e['embedding'], e['metadata'])
    for e in embeddings
]
index.upsert(vectors=vectors)

print("å‘é‡å·²å­˜å…¥Pinecone")

# 4. RAGæŸ¥è¯¢ç¤ºä¾‹
query = "ä½ çš„æŸ¥è¯¢é—®é¢˜"
query_embedding = client.embeddings.create(
    model="text-embedding-3-large",
    input=query
).data[0].embedding

results = index.query(
    vector=query_embedding,
    top_k=5,
    include_metadata=True
)

for match in results['matches']:
    print(f"ç›¸ä¼¼åº¦: {{match['score']}}")
    print(f"å†…å®¹: {{match['metadata']}}")
    print("-" * 50)
'''
    
    async def generate_full_product(self, industry: str, max_docs: int = 10000) -> str:
        """
        ä¸€é”®ç”Ÿæˆå®Œæ•´æ•°æ®äº§å“
        """
        print(f"\n{'='*60}")
        print(f"å¼€å§‹ç”Ÿæˆ {industry} è¡Œä¸šæ•°æ®äº§å“")
        print(f"{'='*60}\n")
        
        # æ­¥éª¤1ï¼šçˆ¬å–
        raw_data = await self.crawl_industry_data(industry, max_docs)
        
        # æ­¥éª¤2ï¼šæ¸…æ´—
        cleaned_data = self.clean_and_structure(raw_data)
        
        # æ­¥éª¤3ï¼šå‘é‡åŒ–
        vectorized_corpus = self.vectorize_for_rag(cleaned_data)
        
        # æ­¥éª¤4ï¼šæ‰“åŒ…
        product_path = self.package_as_product(industry, vectorized_corpus)
        
        print(f"\n{'='*60}")
        print(f"âœ… æ•°æ®äº§å“ç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“¦ äº§å“è·¯å¾„: {product_path}")
        print(f"ğŸ’° å»ºè®®å”®ä»·: {self.target_industries[industry]['ä»·å€¼']}")
        print(f"{'='*60}\n")
        
        return product_path
    
    def get_sales_pitch(self, industry: str) -> str:
        """
        ç”Ÿæˆé”€å”®è¯æœ¯ï¼ˆç”¨äºå†·é‚®ä»¶ï¼‰
        """
        info = self.target_industries.get(industry, {})
        
        return f"""
ä¸»é¢˜ï¼š{industry} è¡Œä¸šé«˜è´¨é‡è®­ç»ƒæ•°æ® - ä¸ºæ‚¨çš„AIèŠ‚çœ3ä¸ªæœˆæ•°æ®å·¥ç¨‹æ—¶é—´

æ‚¨å¥½ï¼Œ

æˆ‘æ³¨æ„åˆ°æ‚¨çš„å›¢é˜Ÿæ­£åœ¨å¼€å‘ {industry} é¢†åŸŸçš„AIäº§å“ã€‚

æˆ‘ä»¬æä¾›å·²æ¸…æ´—ã€ç»“æ„åŒ–ã€å‘é‡åŒ–é¢„å¤„ç†çš„ {industry} è¡Œä¸šè¯­æ–™åº“ï¼š

âœ… {info.get('æ•°æ®æº', [])[0] if info.get('æ•°æ®æº') else 'ä¸“ä¸šæ•°æ®æº'}
âœ… 10,000+ é«˜è´¨é‡æ–‡æ¡£
âœ… ç›´æ¥å¯ç”¨äºRAG/å¾®è°ƒ
âœ… èŠ‚çœæ‚¨3ä¸ªæœˆçš„æ•°æ®å·¥ç¨‹æ—¶é—´

ç—›ç‚¹ï¼š{info.get('ç—›ç‚¹', '')}

å®šä»·ï¼š{info.get('ä»·å€¼', '')}ï¼ˆå«30å¤©æŠ€æœ¯æ”¯æŒï¼‰

æä¾›å…è´¹æ ·æœ¬æ•°æ®ï¼ˆ100æ¡ï¼‰ï¼Œå›å¤å³å¯è·å–ã€‚

æœ€ä½³ï¼Œ
[æ‚¨çš„åå­—]
"""


# ============================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================

async def main():
    engine = DataCorpusEngine()
    
    # ç”ŸæˆåŒ»ç–—åˆè§„æ•°æ®äº§å“
    product_path = await engine.generate_full_product(
        industry="medical_compliance",
        max_docs=10000
    )
    
    # è·å–é”€å”®è¯æœ¯
    pitch = engine.get_sales_pitch("medical_compliance")
    print(pitch)


if __name__ == "__main__":
    asyncio.run(main())

