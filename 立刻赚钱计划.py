"""
ç«‹åˆ»èµšé’±è¡ŒåŠ¨è®¡åˆ’ - åœæ­¢æ¶æ„è‡ªå—¨ï¼Œå»å®Œæˆç¬¬ä¸€ç¬”äº¤æ˜“
"""

# ============================================
# ç°å®æ£€æŸ¥
# ============================================

REALITY_CHECK = """
âŒ é”™è¯¯æ–¹å‘ï¼š
- æ„å»ºå®Œç¾çš„åº•å±‚æ¶æ„
- åšé€šç”¨çš„AIå‘˜å·¥å¹³å°
- å–ç»™å…¶ä»–åˆ›ä¸šè€…

âœ… æ­£ç¡®æ–¹å‘ï¼š
- ç”¨AIå‘˜å·¥ä¸ºè‡ªå·±æ¥å•èµšé’±
- ä¸“æ³¨æ•°æ®å¤„ç†å’Œæé€Ÿäº¤ä»˜
- Upwork/Fiverræ¥å•å˜ç°

æ ¸å¿ƒï¼šå®¢æˆ·ä¸ä¸ºä»£ç ä¹°å•ï¼Œåªä¸ºç»“æœä¹°å•ï¼
"""

# ============================================
# ç«‹åˆ»å¯æ¥çš„å•å­ç±»å‹ï¼ˆ30-100ç¾å…ƒï¼‰
# ============================================

MONEY_MAKING_TASKS = {
    "Web Scrapingï¼ˆç½‘é¡µæŠ“å–ï¼‰": {
        "å…³é”®è¯": "web scraping, data extraction, scrape website",
        "é¢„ç®—": "$30-100",
        "äº¤ä»˜æ—¶é—´": "24-48å°æ—¶",
        "å·¥å…·": "Python + BeautifulSoup + Selenium",
        "ç¤ºä¾‹ä»»åŠ¡": [
            "æŠ“å–ç”µå•†ç½‘ç«™äº§å“æ•°æ®ï¼ˆæ ‡é¢˜ã€ä»·æ ¼ã€è¯„åˆ†ï¼‰",
            "æŠ“å–LinkedInè”ç³»äººä¿¡æ¯",
            "æŠ“å–æˆ¿åœ°äº§ç½‘ç«™æˆ¿æºæ•°æ®",
            "æŠ“å–ç¤¾äº¤åª’ä½“å¸–å­å’Œè¯„è®º"
        ],
        "ç«äº‰ä¼˜åŠ¿": "æä¾›10%æ ·æœ¬æ•°æ®ï¼Œè¯æ˜å·²è·‘é€š",
        "äº¤ä»˜ç‰©": "CSVæ–‡ä»¶ + ç®€å•æ–‡æ¡£"
    },
    
    "Data Cleaningï¼ˆæ•°æ®æ¸…æ´—ï¼‰": {
        "å…³é”®è¯": "data cleaning, data processing, excel cleanup",
        "é¢„ç®—": "$30-80",
        "äº¤ä»˜æ—¶é—´": "12-24å°æ—¶",
        "å·¥å…·": "Python + Pandas",
        "ç¤ºä¾‹ä»»åŠ¡": [
            "æ¸…æ´—Excelè¡¨æ ¼ï¼ˆå»é‡ã€æ ¼å¼åŒ–ã€å¡«å……ç¼ºå¤±å€¼ï¼‰",
            "åˆå¹¶å¤šä¸ªCSVæ–‡ä»¶",
            "æ•°æ®æ ‡å‡†åŒ–å’ŒéªŒè¯",
            "åˆ é™¤é‡å¤è”ç³»äºº"
        ],
        "ç«äº‰ä¼˜åŠ¿": "1å°æ—¶å†…äº¤ä»˜æ ·æœ¬",
        "äº¤ä»˜ç‰©": "æ¸…æ´—åçš„Excel/CSV"
    },
    
    "Data Entryï¼ˆæ•°æ®å½•å…¥ï¼‰": {
        "å…³é”®è¯": "data entry, copy paste, manual data entry",
        "é¢„ç®—": "$20-50",
        "äº¤ä»˜æ—¶é—´": "6-12å°æ—¶",
        "å·¥å…·": "Pythonè‡ªåŠ¨åŒ–è„šæœ¬",
        "ç¤ºä¾‹ä»»åŠ¡": [
            "ä»PDFæå–æ•°æ®åˆ°Excel",
            "ä»å›¾ç‰‡è¯†åˆ«æ–‡å­—å½•å…¥",
            "æ‰¹é‡å¤åˆ¶ç²˜è´´æ•°æ®",
            "è¡¨æ ¼æ•°æ®è¿ç§»"
        ],
        "ç«äº‰ä¼˜åŠ¿": "ç”¨AIè‡ªåŠ¨åŒ–ï¼Œé€Ÿåº¦å¿«10å€",
        "äº¤ä»˜ç‰©": "Excel/Google Sheets"
    },
    
    "Lead Generationï¼ˆçº¿ç´¢ç”Ÿæˆï¼‰": {
        "å…³é”®è¯": "lead generation, email list, contact list",
        "é¢„ç®—": "$50-150",
        "äº¤ä»˜æ—¶é—´": "24-48å°æ—¶",
        "å·¥å…·": "Python + Apollo.io API + Hunter.io",
        "ç¤ºä¾‹ä»»åŠ¡": [
            "ç”Ÿæˆç‰¹å®šè¡Œä¸šçš„å…¬å¸è”ç³»äººåˆ—è¡¨",
            "æ‰¾åˆ°å†³ç­–è€…çš„é‚®ç®±å’ŒLinkedIn",
            "éªŒè¯é‚®ç®±æœ‰æ•ˆæ€§",
            "æŒ‰åœ°åŒº/è¡Œä¸šç­›é€‰æ½œåœ¨å®¢æˆ·"
        ],
        "ç«äº‰ä¼˜åŠ¿": "æä¾›éªŒè¯è¿‡çš„é«˜è´¨é‡æ•°æ®",
        "äº¤ä»˜ç‰©": "CSVæ–‡ä»¶ï¼ˆå…¬å¸åã€è”ç³»äººã€é‚®ç®±ã€LinkedInï¼‰"
    },
    
    "SEO Researchï¼ˆSEOç ”ç©¶ï¼‰": {
        "å…³é”®è¯": "keyword research, competitor analysis, seo audit",
        "é¢„ç®—": "$40-100",
        "äº¤ä»˜æ—¶é—´": "12-24å°æ—¶",
        "å·¥å…·": "Python + Ahrefs API / Semrush",
        "ç¤ºä¾‹ä»»åŠ¡": [
            "å…³é”®è¯ç ”ç©¶å’Œæœç´¢é‡åˆ†æ",
            "ç«å“ç½‘ç«™åˆ†æ",
            "å¤–é“¾æœºä¼šæŒ–æ˜",
            "æŠ€æœ¯SEOå®¡è®¡"
        ],
        "ç«äº‰ä¼˜åŠ¿": "è‡ªåŠ¨åŒ–æŠ¥å‘Šç”Ÿæˆ",
        "äº¤ä»˜ç‰©": "ExcelæŠ¥å‘Š + å¯è§†åŒ–å›¾è¡¨"
    }
}

# ============================================
# æç®€äº¤ä»˜è„šæœ¬ï¼ˆä¸è¦æ¶æ„ï¼Œç›´æ¥å¹²ï¼‰
# ============================================

QUICK_SCRIPTS = {
    "web_scraping_template.py": """
# ç½‘é¡µæŠ“å–æ¨¡æ¿ - å•æ–‡ä»¶ï¼Œç›´æ¥è·‘
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

def scrape_products(url, max_pages=5):
    results = []
    
    for page in range(1, max_pages + 1):
        print(f"æŠ“å–ç¬¬ {page} é¡µ...")
        
        response = requests.get(f"{url}?page={page}")
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ ¹æ®å®é™…ç½‘ç«™è°ƒæ•´é€‰æ‹©å™¨
        products = soup.find_all('div', class_='product-item')
        
        for product in products:
            title = product.find('h3').text.strip()
            price = product.find('span', class_='price').text.strip()
            rating = product.find('span', class_='rating').text.strip()
            
            results.append({
                'title': title,
                'price': price,
                'rating': rating
            })
        
        time.sleep(1)  # ç¤¼è²Œå»¶è¿Ÿ
    
    return results

# æ‰§è¡Œ
url = "https://example.com/products"
data = scrape_products(url)

# ä¿å­˜
df = pd.DataFrame(data)
df.to_csv('scraped_data.csv', index=False)
print(f"å®Œæˆï¼æŠ“å–äº† {len(data)} æ¡æ•°æ®")
""",

    "data_cleaning_template.py": """
# æ•°æ®æ¸…æ´—æ¨¡æ¿ - å•æ–‡ä»¶ï¼Œç›´æ¥è·‘
import pandas as pd

def clean_data(input_file):
    # è¯»å–
    df = pd.read_csv(input_file)
    print(f"åŸå§‹æ•°æ®: {len(df)} è¡Œ")
    
    # å»é‡
    df = df.drop_duplicates()
    print(f"å»é‡å: {len(df)} è¡Œ")
    
    # åˆ é™¤ç©ºè¡Œ
    df = df.dropna(how='all')
    
    # å¡«å……ç¼ºå¤±å€¼
    df = df.fillna('')
    
    # æ ‡å‡†åŒ–æ ¼å¼
    if 'email' in df.columns:
        df['email'] = df['email'].str.lower().str.strip()
    
    if 'phone' in df.columns:
        df['phone'] = df['phone'].str.replace(r'[^0-9]', '', regex=True)
    
    # ä¿å­˜
    output_file = input_file.replace('.csv', '_cleaned.csv')
    df.to_csv(output_file, index=False)
    print(f"æ¸…æ´—å®Œæˆï¼ä¿å­˜åˆ° {output_file}")
    
    return output_file

# æ‰§è¡Œ
clean_data('input.csv')
""",

    "lead_generation_template.py": """
# çº¿ç´¢ç”Ÿæˆæ¨¡æ¿ - å•æ–‡ä»¶ï¼Œç›´æ¥è·‘
import requests
import pandas as pd

def generate_leads(industry, location, count=100):
    # ä½¿ç”¨Apollo.ioæˆ–ç±»ä¼¼API
    # è¿™é‡Œæ˜¯ç¤ºä¾‹ï¼Œéœ€è¦æ›¿æ¢ä¸ºçœŸå®API
    
    leads = []
    
    # æ¨¡æ‹Ÿæ•°æ®ï¼ˆå®é™…ä½¿ç”¨APIï¼‰
    for i in range(count):
        leads.append({
            'company': f'Company {i}',
            'contact_name': f'Person {i}',
            'email': f'person{i}@company{i}.com',
            'linkedin': f'https://linkedin.com/in/person{i}',
            'industry': industry,
            'location': location
        })
    
    # ä¿å­˜
    df = pd.DataFrame(leads)
    df.to_csv('leads.csv', index=False)
    print(f"ç”Ÿæˆäº† {len(leads)} ä¸ªçº¿ç´¢")
    
    return leads

# æ‰§è¡Œ
generate_leads('SaaS', 'San Francisco', 100)
"""
}

# ============================================
# Upworkç«æ ‡æ¨¡æ¿ï¼ˆé™ç»´æ‰“å‡»ï¼‰
# ============================================

PROPOSAL_TEMPLATE = """
Hi [Client Name],

I can deliver this in 24 hours with high quality.

âœ… I've already processed a sample of your data (see attached screenshot)
âœ… 100% accurate extraction/cleaning
âœ… Delivered in CSV/Excel format
âœ… Unlimited revisions until you're satisfied

I use Python automation to ensure:
- Fast delivery (24-48 hours)
- High accuracy (99%+)
- Clean, structured data

Sample data is ready. I can start immediately once you award the project.

Looking forward to working with you!

Best regards,
[Your Name]

P.S. Check the attached sample - this is what you'll get for the full dataset.
"""

# ============================================
# é£ä¹¦æœºå™¨äººé›†æˆï¼ˆæ›¿ä»£Telegramï¼‰
# ============================================

FEISHU_BOT_SETUP = """
# é£ä¹¦æœºå™¨äººé…ç½®

1. åˆ›å»ºé£ä¹¦æœºå™¨äºº
   - è®¿é—® https://open.feishu.cn/
   - åˆ›å»ºä¼ä¸šè‡ªå»ºåº”ç”¨
   - è·å– App ID å’Œ App Secret

2. é…ç½®Webhook
   - æ·»åŠ æœºå™¨äººåˆ°ç¾¤ç»„
   - è·å– Webhook URL

3. å‘é€æ¶ˆæ¯ç¤ºä¾‹
```python
import requests

def send_feishu_message(webhook_url, text):
    data = {
        "msg_type": "text",
        "content": {
            "text": text
        }
    }
    requests.post(webhook_url, json=data)

# ä½¿ç”¨
webhook = "https://open.feishu.cn/open-apis/bot/v2/hook/xxx"
send_feishu_message(webhook, "ä»»åŠ¡å®Œæˆï¼æŠ“å–äº†500æ¡æ•°æ®")
```

4. æ¥æ”¶å‘½ä»¤
```python
from flask import Flask, request

app = Flask(__name__)

@app.route('/feishu', methods=['POST'])
def feishu_webhook():
    data = request.json
    
    # è§£æå‘½ä»¤
    text = data.get('event', {}).get('message', {}).get('content', '')
    
    if 'æŠ“å–' in text:
        # æ‰§è¡ŒæŠ“å–ä»»åŠ¡
        result = scrape_data()
        send_feishu_message(webhook, f"å®Œæˆï¼{result}")
    
    return 'ok'

app.run(port=5000)
```
"""

# ============================================
# ç«‹åˆ»æ‰§è¡Œæ¸…å•ï¼ˆä»Šæ™šï¼‰
# ============================================

IMMEDIATE_ACTION = """
ğŸ”¥ åœæ­¢ä¸€åˆ‡æ¶æ„å¼€å‘ï¼Œç«‹åˆ»æ‰§è¡Œï¼š

ä»Šæ™šï¼ˆ2å°æ—¶ï¼‰ï¼š
1. æ³¨å†ŒUpworkè´¦å·ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
2. æœç´¢å…³é”®è¯ï¼š"web scraping" + "Fixed Price" + "$30-100"
3. æ‰¾åˆ°3ä¸ªä»»åŠ¡
4. ç”¨Pythonå†™å‡º10%æ ·æœ¬æ•°æ®
5. æˆªå›¾æ ·æœ¬æ•°æ®
6. å‘é€ç«æ ‡ï¼ˆé™„ä¸Šæ ·æœ¬æˆªå›¾ï¼‰

æ˜å¤©ï¼ˆ4å°æ—¶ï¼‰ï¼š
1. ç­‰å¾…å›å¤
2. æ¥åˆ°å•åï¼Œç”¨æœ€ä¸‘é™‹ä½†æœ‰æ•ˆçš„æ–¹å¼è·‘å‡ºç»“æœ
3. äº¤ä»˜ï¼Œæ”¶é’±
4. é‡å¤

ç›®æ ‡ï¼š
- ç¬¬1å‘¨ï¼šèµšåˆ°ç¬¬ä¸€ä¸ª$50
- ç¬¬1æœˆï¼šèµšåˆ°$500
- ç¬¬3æœˆï¼šèµšåˆ°$2000/æœˆ

è®°ä½ï¼š
- ä¸è¦ä¼˜åŒ–ä»£ç 
- ä¸è¦é‡æ„æ¶æ„
- ä¸è¦è¿½æ±‚å®Œç¾
- åªè¦èƒ½è·‘å‡ºç»“æœï¼Œäº¤ä»˜æ”¶é’±

å®Œæˆç¬¬ä¸€ç¬”å…¥è´¦ï¼Œç³»ç»Ÿæ‰ç®—çœŸæ­£æˆç«‹ï¼
"""

if __name__ == "__main__":
    print("=" * 80)
    print("åœæ­¢æ¶æ„è‡ªå—¨ï¼Œç«‹åˆ»å»èµšé’±ï¼")
    print("=" * 80)
    print(REALITY_CHECK)
    print("\n" + IMMEDIATE_ACTION)

